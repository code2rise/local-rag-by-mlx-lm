"""
data-processor.py

This script implements a full end-to-end Retrieval-Augmented Generation (RAG) pipeline.
The process begins with a raw PDF document and concludes with the ability to ask questions
and receive contextually-aware answers generated by a Large Language Model (LLM).

The pipeline consists of the following major steps:
1.  PDF Parsing: Opens and extracts raw text from a PDF file page by page.
2.  Text Cleaning and Structuring: Formats the extracted text and organizes it by page.
3.  Text Splitting and Chunking: Uses spaCy to split text into sentences and then groups
    those sentences into smaller, manageable "chunks".
4.  Embedding Generation: Converts the text chunks into numerical vector representations
    (embeddings) using a Sentence Transformer model, leveraging GPU acceleration (MPS).
5.  Similarity Search (Retrieval): Implements a function to find the most relevant text
    chunks for a given query by comparing their embeddings.
6.  Prompt Engineering & Generation (Augmentation): Uses a local LLM (via MLX) to generate
    a human-like answer, using the retrieved chunks as context to inform the response.
"""
# Requires !pip install PyMuPDF, see: https://github.com/pymupdf/pymupdf
import random
import re
import textwrap
from typing import Union

import fitz  # (pymupdf, found this is better than pypdf for our use case, note: licence is AGPL-3.0, keep that in mind if you want to use any code commercially)
import numpy as np
import pandas as pd
from mlx.utils import tree_flatten
from spacy.lang.en import English
from sympy import false
from torch import Tensor
from tqdm.auto import tqdm  # for progress bars, requires !pip install tqdm


def text_formatter(text: str) -> str:
    """
    Performs basic cleaning and formatting on a string of text.

    Specifically, it replaces newline characters with spaces to create a single,
    continuous line of text and removes any leading/trailing whitespace.
    """
    cleaned_text = text.replace("\n", " ").strip() # note: this might be different for each doc (best to experiment)

    # Other potential text formatting functions can go here
    return cleaned_text

# Open PDF and get lines/pages
# Note: this only focuses on text, rather than images/figures etc
def open_and_read_pdf(pdf_path: str) -> list[dict]:
    """
    Opens a PDF file, reads its text content page by page, and collects statistics.
    This function iterates through each page of the specified PDF, extracts the
    raw text, and compiles it into a structured list of dictionaries. Each dictionary
    represents a page and contains its text content along with metadata.

    Args:
        pdf_path (str): The file path to the PDF document.

    Returns:
        list[dict]: A list where each dictionary corresponds to a page in the PDF.
                    Each dictionary includes the page number, text content, and
                    various calculated statistics like character and word counts.
    """
    doc = fitz.open(pdf_path)  # open a document
    pages_and_texts = []
    # Iterate through each page of the document with a progress bar
    for page_number, page in tqdm(enumerate(doc), desc="Reading PDF"):
        text = page.get_text()  # get plain text encoded as UTF-8
        text = text_formatter(text) # Clean the text
        pages_and_texts.append({"page_number": page_number - 41,  # adjust page numbers since our PDF starts on page 42
                                "page_char_count": len(text),
                                "page_word_count": len(text.split(" ")),
                                "page_sentence_count_raw": len(text.split(". ")),
                                "page_token_count": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them
                                "text": text})
    return pages_and_texts

def text_splitter() -> None :
    """
    Splits the text of each page into sentences using spaCy's sentencizer.

    This function modifies the input list of dictionaries in-place by adding a new
    key, "sentences", to each dictionary. This key holds a list of sentence strings
    extracted from the page's text. It also adds a sentence count.

    Args:
        page_and_texts (list[dict]): A list of page dictionaries from `open_and_read_pdf`.
    """
    nlp = English()

    # Add a sentencizer pipeline, see https://spacy.io/api/sentencizer/
    nlp.add_pipe("sentencizer")

    # Process each page's text to identify sentence boundaries
    for item in tqdm(pages_and_texts, desc="Splitting text into sentences"):
        item["sentences"] = list(nlp(item["text"]).sents)

        # Make sure all sentences are strings
        item["sentences"] = [str(sentence) for sentence in item["sentences"]]

        # Count the sentences
        item["page_sentence_count_spacy"] = len(item["sentences"])

    # Inspect an example
    # print(random.sample(pages_and_texts, k=1))
    # df = pd.DataFrame(pages_and_texts)
    # df.describe().round(2)

def chunk_sentences_into_text(page_and_texts: list[dict]) -> None:
    """
    Groups sentences on each page into smaller "chunks".

    This function modifies the input list in-place. It iterates through each page's
    list of sentences and groups them into sublists (chunks) of a predefined size.
    This is a common strategy in RAG to create contextually relevant but manageable
    pieces of text for embedding.

    Args:
        page_and_texts (list[dict]): A list of page dictionaries, processed by `textSplitter`.
    """
    # Define split size to turn groups of sentences into chunks
    num_sentence_chunk_size = 10

    # Create a function that recursively splits a list into desired sizes
    def split_list(input_list: list,
                   slice_size: int) -> list[list[str]]:
        """
        Splits the input_list into sublists of size slice_size (or as close as possible).

        For example, a list of 17 sentences would be split into two lists of [[10], [7]]
        """
        return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]

    # Loop through pages and texts and split sentences into chunks
    for item in tqdm(pages_and_texts, desc="Chunking sentences"):
        item["sentence_chunks"] = split_list(input_list=item["sentences"],
                                             slice_size=num_sentence_chunk_size)
        item["num_chunks"] = len(item["sentence_chunks"])

    # Sample an example from the group (note: many samples have only 1 chunk as they have <=10 sentences total)
    print("Chunkd text : ", random.sample(pages_and_texts, k=1))

def organiseChunks(page_and_texts: list[dict]) -> list[dict]:
    """
    Flattens the chunked data structure and filters out small chunks.

    This function transforms the data from a list of pages (each with multiple chunks)
    into a single flat list of chunks. Each item in the new list is a dictionary
    representing one chunk, containing its text, original page number, and stats.
    It also filters out chunks that are too short (based on token count) to be
    meaningfully useful as context.

    Args:
        page_and_texts (list[dict]): A list of page dictionaries, processed by `chunkSentencesIntoText`.

    Returns:
        list[dict]: A filtered list of chunk dictionaries, where each chunk exceeds a minimum token length.
    """
    # Flatten the list of pages with chunks into a list of individual chunks
    pages_and_chunks = []
    for item in tqdm(pages_and_texts, desc="Organizing and filtering chunks"):
        for sentence_chunk in item["sentence_chunks"]:
            chunk_dict = {}
            chunk_dict["page_number"] = item["page_number"]

            # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)
            joined_sentence_chunk = "".join(sentence_chunk).replace("  ", " ").strip()
            # Fix spacing issues after periods (e.g., ".A" -> ". A")
            joined_sentence_chunk = re.sub(r'\.([A-Z])', r'. \1', joined_sentence_chunk) # ".A" -> ". A" for any full-stop/capital letter combo
            chunk_dict["sentence_chunk"] = joined_sentence_chunk

            # Get stats about the chunk
            chunk_dict["chunk_char_count"] = len(joined_sentence_chunk)
            chunk_dict["chunk_word_count"] = len([word for word in joined_sentence_chunk.split(" ")])
            chunk_dict["chunk_token_count"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters

            pages_and_chunks.append(chunk_dict)

    # --- Analysis and Filtering ---

    # View a random sample for inspection
    # print(random.sample(pages_and_chunks, k=1))

    # Get stats about our chunks
    df = pd.DataFrame(pages_and_chunks)
    # print(df.describe().round(2))

    # Filter out chunks that are too short to be useful.
    min_token_length = 30
    # Show some examples of chunks being filtered out
    # for row in df[df["chunk_token_count"] <= min_token_length].sample(5).iterrows():
    #     print(f'Chunk token count: {row[1]["chunk_token_count"]} | Text: {row[1]["sentence_chunk"]}')

    pages_and_chunks_over_min_token_len = df[df["chunk_token_count"] > min_token_length].to_dict(orient="records")
    # print(pages_and_chunks_over_min_token_len[:2])
    return pages_and_chunks_over_min_token_len

def createEmbeddingOfTextChunks(pages_and_chunks_over_min_token_len: list[dict]):
    """
    Generates vector embeddings for each text chunk and saves them to a CSV file.

    This function uses a Sentence Transformer model to convert each text chunk into a
    high-dimensional numerical vector (embedding). It processes chunks one by one and
    adds the resulting embedding to its corresponding dictionary. Finally, it saves
    the entire dataset, including the embeddings, to a CSV file for persistence.

    Args:
        pages_and_chunks_over_min_token_len (list[dict]): The filtered list of chunks
            from the `organiseChunks` function.
    """
    from sentence_transformers import SentenceTransformer
    # Load a pre-trained Sentence Transformer model.
    # "all-mpnet-base-v2" is a strong general-purpose model.
    # The device is set to "mps" to leverage Apple Silicon GPU for faster processing.
    embedding_model = SentenceTransformer(model_name_or_path="all-mpnet-base-v2",
                                          device="mps") # choose the device to load the model to (note: GPU will often be *much* faster than CPU)

    # --- Batch vs. One-by-One Embedding ---
    # The commented-out code below shows how to perform batch embedding, which is
    # significantly more efficient than processing one by one.
    # text_chunks = [item["sentence_chunk"] for item in pages_and_chunks_over_min_token_len]
    # text_chunk_embeddings = embedding_model.encode(text_chunks,
    #                                                batch_size=32, # you can use different batch sizes here for speed/performance, I found 32 works well for this use case
    #                                                convert_to_tensor=True) # optional to return embeddings as tensor instead of array
    # The current implementation processes chunks individually, which is simpler to read
    # but slower for large datasets.
    for item in tqdm(pages_and_chunks_over_min_token_len, desc="Creating embeddings"):
        item["embedding"] = embedding_model.encode(item["sentence_chunk"])

    # Save the processed data with embeddings to a CSV file for later use.
    # This prevents needing to re-process the PDF and re-calculate embeddings every time.
    text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)
    embeddings_df_save_path = "text_chunks_and_embeddings_df.csv"
    text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)

    # Import saved file and view
    # text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)
    # print(text_chunks_and_embedding_df_load.head())

def importEmbeddingsIntoTensor() -> Tensor:
    """
    Loads embeddings from CSV, performs a similarity search, and prints results.

    This function demonstrates the "retrieval" part of RAG. It loads the previously
    saved text chunks and embeddings, converts the embeddings into a PyTorch tensor
    for efficient computation, and then performs a similarity search to find the
    most relevant text chunks for a sample query.

    Returns:
        torch.Tensor: A tensor containing all the document chunk embeddings,
                      loaded onto the appropriate device (MPS or CUDA).
    """
    # --- 1. Load and Prepare Data ---
    device = "cuda" if torch.cuda.is_available() else "mps"

    # Import texts and embedding df
    text_chunks_and_embedding_df = pd.read_csv("text_chunks_and_embeddings_df.csv")

    # Convert embedding column back to np.array (it got converted to string when it got saved to CSV)
    # The lambda function parses the string representation of the array.
    text_chunks_and_embedding_df["embedding"] = text_chunks_and_embedding_df["embedding"].apply(lambda x: np.fromstring(x.strip("[]"), sep=" "))

    # Convert texts and embedding df to list of dicts
    pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient="records")

    # Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)
    embeddings = torch.tensor(np.array(text_chunks_and_embedding_df["embedding"].tolist()), dtype=torch.float32).to(device)
    # print(embeddings.shape)

    # --- 2. Perform Similarity Search (Retrieval) ---
    from sentence_transformers import util, SentenceTransformer

    embedding_model = SentenceTransformer(model_name_or_path="all-mpnet-base-v2",
                                      device=device) # choose the device to load the model to

    # Define the user's query
    # Note: This could be anything. But since we're working with a nutrition textbook, we'll stick with nutrition-based queries.
    # query = "macronutrients functions"
    query = "different types of minerals"
    print(f"Query: {query}")

    # Embed the query to the same numerical space as the text examples
    # Note: It's important to embed your query with the same model you embedded your examples with.
    query_embedding = embedding_model.encode(query, convert_to_tensor=True)

    # Get similarity scores with the dot product (we'll time this for fun)
    from time import perf_counter as timer

    start_time = timer()
    dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]
    end_time = timer()

    print(f"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.")

    # Get the top-k results (k=5)
    top_results_dot_product = torch.topk(dot_scores, k=5)
    print(top_results_dot_product)

    # --- 3. Display Results ---
    print(f"Query: '{query}'\n")
    print("Results:")
    # Loop through zipped together scores and indicies from torch.topk
    for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):
        print(f"Score: {score:.4f}")
        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)
        print("Text:")
        print_wrapped(pages_and_chunks[idx]["sentence_chunk"])
        # Print the page number too so we can reference the textbook further (and check the results)
        print(f"Page number: {pages_and_chunks[idx]['page_number']}")
        print("\n")

    return embeddings


# Define helper function to print wrapped text
def print_wrapped(text, wrap_length=80):
    """Prints text with word wrapping to a specified line length."""
    wrapped_text = textwrap.fill(text, wrap_length)
    print(wrapped_text)

import torch
import psutil

def format_size(bytes_val: int) -> str:
    """Format bytes into human-readable MB/GB."""
    for unit in ["B", "KB", "MB", "GB", "TB"]:
        if bytes_val < 1024:
            return f"{bytes_val:.2f} {unit}"
        bytes_val /= 1024
    return f"{bytes_val:.2f} PB"

def get_device_memory_info(device: str = None, human_readable: bool = True):
    """
    Returns memory info for CUDA or MPS.

    Args:
        device: "cuda" | "mps" | None (auto-detect).
        human_readable: if True, values are returned as strings (e.g. '1.25 GB').

    Returns:
        dict with backend, total, allocated, reserved memory.
    """
    if device is None:
        if torch.cuda.is_available():
            device = "cuda"
        elif torch.backends.mps.is_available():
            device = "mps"
        else:
            return {"error": "No CUDA or MPS device available."}

    if device == "cuda":
        props = torch.cuda.get_device_properties(0)
        total = props.total_memory
        allocated = torch.cuda.memory_allocated(0)
        reserved = torch.cuda.memory_reserved(0)

    elif device == "mps":
        total = psutil.virtual_memory().total  # Unified system RAM
        allocated = torch.mps.current_allocated_memory()
        reserved = torch.mps.driver_allocated_memory()

    else:
        return {"error": f"Unsupported device: {device}"}

    if human_readable:
        return {
            "backend": device,
            "total": format_size(total),
            "allocated": format_size(allocated),
            "reserved": format_size(reserved),
        }
    else:
        return {
            "backend": device,
            "total_bytes": total,
            "allocated_bytes": allocated,
            "reserved_bytes": reserved,
        }


def setup_llm() -> str :
    gpu_memory_gb = psutil.virtual_memory().total  # Unified system RAM

    # Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.
    if gpu_memory_gb < 5.1:
        print(f"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.")
    elif gpu_memory_gb < 8.1:
        print(f"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.")
        use_quantization_config = True
        model_id = "google/gemma-2b-it"
    elif gpu_memory_gb < 19.0:
        print(f"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.")
        use_quantization_config = False
        model_id = "google/gemma-2b-it"
    elif gpu_memory_gb > 19.0:
        print(f"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.")
        use_quantization_config = False
        # model_id = "google/gemma-7b-it"
        model_id = "/Users/rise/.cache/huggingface/hub/gemma1-7b-it-q5"

    print(f"use_quantization_config set to: {use_quantization_config}")
    print(f"model_id set to: {model_id}")
    return model_id

def use_llm_mlx(model_id: str,
                pages_and_chunks: list[dict],
                embeddings: Union[torch.Tensor, np.ndarray]) :
    """
    Uses a local LLM with MLX to perform Retrieval-Augmented Generation (RAG).

    This function takes a user query, retrieves relevant context from the document
    embeddings, formats a detailed prompt with this context, and then uses the
    LLM to generate a final answer.

    Args:
        model_id (str): The path or Hugging Face ID of the MLX-compatible model.
        pages_and_chunks (list[dict]): The list of all document chunks.
        embeddings (Union[torch.Tensor, np.ndarray]): The tensor of all chunk embeddings.
    """
    from mlx_lm import load, generate

    # 1. Path to your quantized model (after running mlx_lm.convert)
    # Example: mlx_lm.convert --hf-path google/gemma-7b-it --mlx-path ~/models/gemma1-7b-it-q4 -q --q-bits 4
    # model_path = "/Users/rise/models/gemma1-7b-it-q4"

    # --- 1. Load Model and Tokenizer ---
    print(model_id)
    model, tokenizer = load(model_id)

    # --- 2. Simple Generation (Without RAG) ---
    input_text = "What are the macronutrients, and what roles do they play in the human body?"
    print(f"Input text:\n{input_text}")

    # Create prompt template for instruction-tuned model
    dialogue_template = [
        {"role": "user",
         "content": input_text}
    ]

    # Define a prompt
    # Apply the chat template
    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,
                                           tokenize=False, # keep as raw text (not tokenized)
                                           add_generation_prompt=True)
    print(f"\nPrompt (formatted):\n{prompt}")

    sampling_options = {"max_tokens": 120, "temperature": 0.7, "top_p": 0.9, "top_k": 50, "repetition_penalty": 1.1}

    # Generate output
    out = generate(model, tokenizer, prompt=prompt, verbose=false, formatter=None)

    print("=== Prompt ===")
    print(prompt)
    print("=== Output ===")
    print(out)

    # --- 3. RAG-Enhanced Generation ---
    # Nutrition-style questions generated with GPT4
    gpt4_questions = [
        "What are the macronutrients, and what roles do they play in the human body?",
        "How do vitamins and minerals differ in their roles and importance for health?",
        "Describe the process of digestion and absorption of nutrients in the human body.",
        "What role does fibre play in digestion? Name five fibre containing foods.",
        "Explain the concept of energy balance and its importance in weight management."
    ]

    # Manually created question list
    manual_questions = [
        "How often should infants be breastfed?",
        "What are symptoms of pellagra?",
        "How does saliva help with digestion?",
        "What is the RDI for protein per day?",
        "water soluble vitamins"
    ]

    query_list = gpt4_questions + manual_questions

    # Select a random query to demonstrate the RAG process
    query = random.choice(query_list)
    print(f"Query: {query}")

    # a. Retrieve relevant resources (context) from the document
    scores, indices = retrieve_relevant_resources(query=query,
                                                  embeddings=embeddings)

    # Create a list of context items
    context_items = [pages_and_chunks[i] for i in indices]

    # b. Augment: Format a detailed prompt including the retrieved context
    prompt = prompt_formatter(query=query,
                              context_items=context_items,
                              tokenizer=tokenizer)
    print(prompt)

    # c. Generate: Use the LLM to generate an answer based on the augmented prompt
    outputs = generate(model, tokenizer, prompt, verbose=false, formatter=None)

    print(f"Query: {query}")
    print(f"RAG answer:\n{outputs.replace(prompt, '')}")



def get_model_num_params(model: torch.nn.Module):
    """Calculates the total number of parameters in an MLX model."""
    param_list = model.parameters()

    # flatten and sum sizes
    total_params = sum(v.size for _, v in tree_flatten(param_list))

    print(f"Total parameters: {total_params:,}")

    # CUDA style
    # return sum([param.numel() for param in model.parameters()])

def get_model_mem_size(model: torch.nn.Module):
    """
    Get how much memory a PyTorch model takes up.

    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822
    """
    # Get model parameters and buffer sizes
    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])
    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])

    # Calculate various model sizes
    model_mem_bytes = mem_params + mem_buffers # in bytes
    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes
    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes

    return {"model_mem_bytes": model_mem_bytes,
            "model_mem_mb": round(model_mem_mb, 2),
            "model_mem_gb": round(model_mem_gb, 2)}

def prompt_formatter(query: str,
                     context_items: list[dict],
                     tokenizer) -> str:
    """
    Augments query with text-based context from context_items.
    """
    # Join context items into one dotted paragraph
    context = "- " + "\n- ".join([item["sentence_chunk"] for item in context_items])

    # Create a base prompt with examples to help the model
    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.
    # We could also write this in a txt file and import it in if we wanted.
    base_prompt = """Based on the following context items, please answer the query.
Give yourself room to think by extracting relevant passages from the context before answering the query.
Don't return the thinking, only return the answer.
Make sure your answers are as explanatory as possible.
Use the following examples as reference for the ideal answer style.
\nExample 1:
Query: What are the fat-soluble vitamins?
Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.
\nExample 2:
Query: What are the causes of type 2 diabetes?
Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.
\nExample 3:
Query: What is the importance of hydration for physical performance?
Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.
\nNow use the following context items to answer the user query:
{context}
\nRelevant passages: <extract relevant passages from the context here>
User query: {query}
Answer:"""

    # Update base prompt with context items and query
    base_prompt = base_prompt.format(context=context, query=query)

    # Create prompt template for instruction-tuned model
    dialogue_template = [
        {"role": "user",
         "content": base_prompt}
    ]

    # Apply the chat template
    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,
                                           tokenize=False,
                                           add_generation_prompt=True)
    return prompt

from typing import List, Dict, Tuple, Optional
import torch
from sentence_transformers import SentenceTransformer, util
from time import perf_counter as timer

def retrieve_relevant_resources(
        query: str,
        embeddings: torch.Tensor,
        model: Optional[SentenceTransformer] = None,
        n_resources_to_return: int = 5,
        use_cosine: bool = True,
        clamp_k: bool = True,
        print_time: bool = True,
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Embed `query` and return top-k (scores, indices) w.r.t. `embeddings`.

    Args:
        query: string query
        embeddings: torch.Tensor of shape (N, D) or numpy array of same shape
        model: SentenceTransformer instance. If None, a new model will be loaded with device auto-detected.
        n_resources_to_return: number of top results to return
        use_cosine: use cosine similarity (recommended) if True; use dot product otherwise
        clamp_k: if True, will reduce k to `len(embeddings)` to avoid torch.topk error
        print_time: whether to print similarity computation time

    Returns:
        scores (torch.Tensor shape (k,)), indices (torch.Tensor shape (k,))
    """
    # Validate embeddings type & convert to torch tensor on proper device
    if isinstance(embeddings, np.ndarray):
        embeddings = torch.tensor(embeddings, dtype=torch.float32)

    if not isinstance(embeddings, torch.Tensor):
        raise TypeError("embeddings must be a torch.Tensor or numpy.ndarray")

    # determine target device from embeddings
    target_device = embeddings.device

    # If no model provided, instantiate one on the correct device.
    # NOTE: you may prefer to pass your existing SentenceTransformer instance to avoid re-loading.
    if model is None:
        # default to same model used elsewhere in your script
        model = SentenceTransformer("all-mpnet-base-v2", device=str(target_device))

    # Encode the query
    # SentenceTransformer.encode supports device=... in newer versions; convert_to_tensor returns a torch tensor
    query_embedding = model.encode(query, convert_to_tensor=True)

    # Move query embedding to same device as embeddings
    if query_embedding.device != target_device:
        try:
            query_embedding = query_embedding.to(target_device)
        except Exception:
            # fallback: move embeddings to query device (rare)
            embeddings = embeddings.to(query_embedding.device)
            target_device = query_embedding.device

    # Ensure shapes - util functions accept (D,) or (1, D) as query
    if query_embedding.ndim == 2 and query_embedding.size(0) == 1:
        query_embedding = query_embedding.squeeze(0)

    # compute similarity
    start = timer()
    if use_cosine:
        # cosine similarity returns shape (1, N) for a single query
        sims = util.cos_sim(query_embedding, embeddings)  # torch.Tensor
    else:
        sims = util.dot_score(query_embedding, embeddings)
    # flatten to 1D scores
    scores_1d = sims.view(-1)
    end = timer()

    if print_time:
        print(f"[INFO] similarity computation on {embeddings.size(0)} embeddings: {end - start:.5f} s")

    # clamp k
    N = embeddings.size(0)
    k = int(n_resources_to_return)
    if clamp_k and k > N:
        k = N

    if k <= 0:
        return torch.tensor([]), torch.tensor([], dtype=torch.long)

    # topk (returns values, indices)
    top_scores, top_indices = torch.topk(scores_1d, k=k)

    # move results to CPU for easier further processing
    return top_scores.detach().cpu(), top_indices.detach().cpu()


def print_top_results_and_scores(
        query: str,
        embeddings: torch.Tensor,
        pages_and_chunks: List[Dict],
        model: Optional[SentenceTransformer] = None,
        n_resources_to_return: int = 5,
        use_cosine: bool = True,
        wrap_length: int = 80,
) -> None:
    """
    Retrieve top results for `query` and print the sentence chunks and page numbers.

    Args:
        query: text query
        embeddings: torch.Tensor or np.ndarray of shape (N, D)
        pages_and_chunks: list of dicts where each dict contains at least:
                          - "sentence_chunk": str
                          - "page_number": int
        model: SentenceTransformer instance (recommended to pass your existing model)
        n_resources_to_return: how many results to print
        use_cosine: whether to use cosine similarity
        wrap_length: line wrap width for printing chunks
    """
    scores, indices = retrieve_relevant_resources(
        query=query,
        embeddings=embeddings,
        model=model,
        n_resources_to_return=n_resources_to_return,
        use_cosine=use_cosine,
        print_time=True,
    )

    print(f"\nQuery: {query}\n")
    print("Results:")

    # indices is a 1D torch tensor on CPU; convert to python ints
    indices_list = indices.tolist() if indices.numel() > 0 else []

    for i, idx in enumerate(indices_list):
        # safe bounds check before indexing pages_and_chunks
        if not (0 <= idx < len(pages_and_chunks)):
            print(f"[WARN] index {idx} out of range for pages_and_chunks (len={len(pages_and_chunks)})")
            continue

        # score is a torch scalar in scores (already on CPU)
        score_val = float(scores[i].item()) if isinstance(scores, torch.Tensor) and scores.numel() > i else float(scores[i])

        print(f"Rank {i+1} | Score: {score_val:.4f}")
        chunk_text = pages_and_chunks[idx].get("sentence_chunk", "<no sentence_chunk>")
        page_num = pages_and_chunks[idx].get("page_number", "N/A")

        # print wrapped chunk text (you already have print_wrapped in your script)
        print_wrapped(chunk_text, wrap_length)
        print(f"Page number: {page_num}\n")

if __name__ == "__main__":
    # This block orchestrates the entire RAG pipeline from start to finish.

    # --- Step 1: Data Ingestion and Preparation ---
    pdf_path = "human-nutrition-text.pdf"
    # Read the PDF and extract text into structured pages
    pages_and_texts = open_and_read_pdf(pdf_path)

    # Split the text on each page into sentences
    text_splitter()

    # Group sentences into manageable chunks
    chunk_sentences_into_text(pages_and_texts)

    # Flatten the data structure into a single list of chunks and filter out small ones
    pages_and_chunks_over_min_token_len = organiseChunks(pages_and_texts)

    # --- Step 2: Embedding and Storage ---
    # Create vector embeddings for each chunk and save them to a CSV file
    createEmbeddingOfTextChunks(pages_and_chunks_over_min_token_len)

    # --- Step 3: Retrieval and Generation ---
    # Load embeddings and demonstrate a similarity search
    embeddings = importEmbeddingsIntoTensor()

    # Set up the local LLM based on available system memory
    model_id = setup_llm()

    # Run the full RAG pipeline: retrieve context and generate an answer with the LLM
    use_llm_mlx(model_id, pages_and_chunks_over_min_token_len, embeddings)
